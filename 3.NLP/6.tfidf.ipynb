{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHW3: Vector Semantics and Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, you will implement TF-IDF (Term Frequency Inverse Document Frequency) to produce vector representations of words and see how it captures information about their context. You will also explore the vector space spanned by pretrained embeddings and discover some useful relations.\n",
    "\n",
    "The homework is due on __Friday, 28th September 2018 at 11:59pm__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will implement TF-IDF. Let's import the required libraries and load the data for preparing our word vectors. We are going to load a list of movie plot summaries (http://www.cs.cmu.edu/~ark/personas/) and use that as our corpus. You don't need to perform any preprocessing; the data has been cleaned and tokenized for you already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from random import shuffle\n",
    "import codecs\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Loads the data and returns a list of summaries.\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "    with open('plot_summaries_tokenized.csv') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        summaries = [data['SUMMARY'].strip() for data in reader]\n",
    "    return summaries\n",
    "\n",
    "SUMMARIES = load_data()\n",
    "SUMMARY_COUNT = len(SUMMARIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have ~42000 summaries containing ~13000000 words. We will now proceed by creating a vocabulary and will limit its size to something computationally feasible. You may find python's collections.Counter function useful. You may not import any additional libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Creating a Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 50000\n",
    "def create_vocabulary(documents):\n",
    "    \"\"\"\n",
    "    This function takes in a list of documents and returns a\n",
    "    vocabulary and word list of most frequently appearing VOCAB_SIZE words.\n",
    "    \n",
    "    :param documents: list of strings\n",
    "    :return word_list: A list most frequently appearing unique VOCAB_SIZE words.\n",
    "    :return vocab: A dictionary where key is the word and value is the word's index in word_list\n",
    "    \n",
    "    \"\"\"\n",
    "    word_list = []\n",
    "    vocab = {}\n",
    "    \n",
    "    #your code goes here\n",
    "    \n",
    "    cnt = Counter()\n",
    "    for string in documents:\n",
    "        for word in string.split(\" \"):\n",
    "            cnt[word] += 1\n",
    "            \n",
    "    word_list = [pair[0] for pair in cnt.most_common(VOCAB_SIZE)]\n",
    "    \n",
    "    for i, key in enumerate(cnt):\n",
    "        if key in word_list:\n",
    "            vocab[key] = word_list.index(key)\n",
    "    \n",
    "    return word_list,vocab\n",
    "\n",
    "WORDS,VOCAB = create_vocabulary(SUMMARIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Calculate Term Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following from [ยง6.3.1 Jurafsky & Martin](https://web.stanford.edu/~jurafsky/slp3/6.pdf), for a term frequency matrix $tf$ each element $tf_{t,d}$ is defined as follows \n",
    "\n",
    "$$ \\begin{equation*}\n",
    "    tf_{t,d} = \\begin{cases}\n",
    "               1 + log_{10}\\text{count(t,d)}              & \\text{if count}(t,d) > 0\\\\\n",
    "               0 & \\text{otherwise}\n",
    "           \\end{cases}\n",
    "\\end{equation*} $$\n",
    "\n",
    "Instead of considering the entire document for obtaining the context of a single word, it is more common to use a smaller context. Like in [ยง6.3.2 Jurafsky & Martin](https://web.stanford.edu/~jurafsky/slp3/6.pdf), instead of a __term-document__ matrix, we will calculate the __term-context__ matrix for all the words in the vocabulary.\n",
    "\n",
    "For a word $w_{i}$ and window size $k$, we define the context of $w_i$, $C(w_i,k)$ as the window of k words before and after that word in all the documents. If the defined window exceeds the document length, only consider the the portion till the beginning/end of the document.\n",
    "\n",
    "We define a single element of the __term-context__ matrix, \n",
    "$$ \\begin{equation*}\n",
    "    tc[w_1][w_2] = \\begin{cases}\n",
    "               1 + log_{10}\\text{count}(w_2,C(w_1,k))              & \\text{if count}(w_2,C(w_1,k)) > 0\\\\\n",
    "               0 & \\text{otherwise}\n",
    "           \\end{cases}\n",
    "\\end{equation*} $$\n",
    "\n",
    "![term-context.png](attachment:term-context.png)\n",
    "\n",
    "\n",
    "Complete the function below that calculates the matrix. You may not import any additional libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def term_frequencies(documents,vocab,k):\n",
    "    \"\"\"\n",
    "    This function returns a dictionary of dictionary with each \n",
    "    tf[w1][w2] = 1 + log(number of times the word w2 occurs in context of word w1. (where w1,w2 belong to the vocab)) \n",
    "    or 0 (as defined above)\n",
    "    You may find python's collection.defaultdict useful here\n",
    "    \n",
    "    :param documents: list of strings\n",
    "    :param vocab: the vocabulary with vocab[word] = index\n",
    "    :return tf: A dictionary where tf[word1][word2] = count of word2's occurences in word1's context window\n",
    "    \"\"\"\n",
    "    tf = {}\n",
    "    #your code goes here\n",
    "    \n",
    "    # convert list of string to list of word\n",
    "    for idx, string in enumerate(documents):\n",
    "        w_list = string.split(\" \")\n",
    "        # context for term with window k\n",
    "        for i in range(len(w_list)):\n",
    "            if w_list[i] in vocab:\n",
    "                if i < k:\n",
    "                    context_list = w_list[0:i+k+1]\n",
    "                else:\n",
    "                    context_list = w_list[i-k:i+k+1]\n",
    "                \n",
    "                for word in context_list:\n",
    "                    if context_list.index(word) != context_list.index(w_list[i]) and word in vocab:\n",
    "                        if w_list[i] not in tf:\n",
    "                            tf[w_list[i]] = defaultdict(int)\n",
    "                        tf[w_list[i]][word] += 1\n",
    "\n",
    "    for context, word_dict in tf.items():\n",
    "        for word, count in word_dict.items():\n",
    "            word_dict[word] = 1 + math.log10(count)\n",
    "    \n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Lyosha': defaultdict(int,\n",
       "             {'a': 1.0, 'and': 1.0, 'driver': 1.0, 'saxophonist': 1.0}),\n",
       " 'Shlykov': defaultdict(int, {'a': 1.0, 'hardworking': 1.0}),\n",
       " 'a': defaultdict(int,\n",
       "             {'Lyosha': 1.0,\n",
       "              'Shlykov': 1.0,\n",
       "              'and': 1.0,\n",
       "              'bizarre': 1.0,\n",
       "              'develop': 1.3010299956639813,\n",
       "              'hardworking': 1.0,\n",
       "              'lovehate': 1.0,\n",
       "              'saxophonist': 1.3010299956639813,\n",
       "              'taxi': 1.0}),\n",
       " 'after': defaultdict(int, {'all': 1.0, 'different': 1.0, 'so': 1.0}),\n",
       " 'all': defaultdict(int, {'after': 1.0, 'different': 1.0}),\n",
       " 'and': defaultdict(int,\n",
       "             {'Lyosha': 1.0,\n",
       "              'a': 1.0,\n",
       "              'despite': 1.0,\n",
       "              'driver': 1.0,\n",
       "              'lovehate': 1.0,\n",
       "              'relationship': 1.0,\n",
       "              'taxi': 1.0,\n",
       "              'their': 1.0}),\n",
       " 'arent': defaultdict(int,\n",
       "             {'different': 1.0, 'realize': 1.0, 'so': 1.0, 'they': 1.0}),\n",
       " 'bizarre': defaultdict(int,\n",
       "             {'a': 1.0, 'develop': 1.0, 'lovehate': 1.0, 'relationship': 1.0}),\n",
       " 'despite': defaultdict(int,\n",
       "             {'and': 1.0,\n",
       "              'prejudices': 1.0,\n",
       "              'relationship': 1.0,\n",
       "              'their': 1.0}),\n",
       " 'develop': defaultdict(int,\n",
       "             {'a': 1.3010299956639813, 'bizarre': 1.0, 'saxophonist': 1.0}),\n",
       " 'different': defaultdict(int,\n",
       "             {'after': 1.0, 'all': 1.0, 'arent': 1.0, 'so': 1.0}),\n",
       " 'driver': defaultdict(int,\n",
       "             {'Lyosha': 1.0, 'and': 1.0, 'hardworking': 1.0, 'taxi': 1.0}),\n",
       " 'hardworking': defaultdict(int,\n",
       "             {'Shlykov': 1.0, 'a': 1.0, 'driver': 1.0, 'taxi': 1.0}),\n",
       " 'lovehate': defaultdict(int,\n",
       "             {'a': 1.0, 'and': 1.0, 'bizarre': 1.0, 'relationship': 1.0}),\n",
       " 'prejudices': defaultdict(int,\n",
       "             {'despite': 1.0, 'realize': 1.0, 'their': 1.0, 'they': 1.0}),\n",
       " 'realize': defaultdict(int,\n",
       "             {'arent': 1.0, 'prejudices': 1.0, 'their': 1.0, 'they': 1.0}),\n",
       " 'relationship': defaultdict(int,\n",
       "             {'and': 1.0, 'bizarre': 1.0, 'despite': 1.0, 'lovehate': 1.0}),\n",
       " 'saxophonist': defaultdict(int,\n",
       "             {'Lyosha': 1.0, 'a': 1.3010299956639813, 'develop': 1.0}),\n",
       " 'so': defaultdict(int,\n",
       "             {'after': 1.0, 'arent': 1.0, 'different': 1.0, 'they': 1.0}),\n",
       " 'taxi': defaultdict(int,\n",
       "             {'a': 1.0, 'and': 1.0, 'driver': 1.0, 'hardworking': 1.0}),\n",
       " 'their': defaultdict(int,\n",
       "             {'and': 1.0, 'despite': 1.0, 'prejudices': 1.0, 'realize': 1.0}),\n",
       " 'they': defaultdict(int,\n",
       "             {'arent': 1.0, 'prejudices': 1.0, 'realize': 1.0, 'so': 1.0})}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST CASE\n",
    "word_test, vocab_test = create_vocabulary(SUMMARIES[:1])\n",
    "tf_test = term_frequencies(SUMMARIES[:1], vocab_test , 2)\n",
    "tf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 5\n",
    "tf_matrix = term_frequencies(SUMMARIES,VOCAB,WINDOW_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Calculate Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now calculate the document frequency vector. The document frequency for a word is the number of documents (__contexts__ in our case) that word appears in. For a word $t$, window size $k$ and context $C(t,k)$, the Document Frequency $df_t$ is defined as number of contexts, the word $t$ occurs in.\n",
    "\n",
    "Complete the function below that calculates the $df$ vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def document_frequencies(tf_matrix):\n",
    "    \"\"\"\n",
    "    This function takes in a tf-matrix and returns a dictionary\n",
    "    with df scores for each word in the matrix.\n",
    "    \n",
    "    :param tf: A dictionary where tf[w1][w2] = 1 + log(number of times the word w2 occurs in context of word w1. (where w1,w2 belong to the vocab)) \n",
    "    or 0 (as defined above)\n",
    "    :return df_scores: A dictionary with df_scores[word] = the df score of the word defined as above\n",
    "    \"\"\"\n",
    "    df_scores = {}\n",
    "    \n",
    "    #your code goes here\n",
    "    \n",
    "    for context, word_dict in tf_matrix.items():\n",
    "        for word, tf in word_dict.items():\n",
    "            if word not in df_scores:\n",
    "                df_scores[word] = 1\n",
    "            else:\n",
    "                df_scores[word] += 1\n",
    "\n",
    "    return df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Lyosha': 4,\n",
       " 'Shlykov': 2,\n",
       " 'a': 9,\n",
       " 'after': 3,\n",
       " 'all': 2,\n",
       " 'and': 8,\n",
       " 'arent': 4,\n",
       " 'bizarre': 4,\n",
       " 'despite': 4,\n",
       " 'develop': 3,\n",
       " 'different': 4,\n",
       " 'driver': 4,\n",
       " 'hardworking': 4,\n",
       " 'lovehate': 4,\n",
       " 'prejudices': 4,\n",
       " 'realize': 4,\n",
       " 'relationship': 4,\n",
       " 'saxophonist': 3,\n",
       " 'so': 4,\n",
       " 'taxi': 4,\n",
       " 'their': 4,\n",
       " 'they': 4}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST CASE\n",
    "df_test = document_frequencies(tf_test)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_vector = document_frequencies(tf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 TF-IDF Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following from [ยง6.5 Jurafsky & Martin](https://web.stanford.edu/~jurafsky/slp3/6.pdf), we will now calculate our TF-IDF matrix by re-weighing the term frequencies with the inverse document frequencies. For a word $t$ and $N$ documents, $idf_t$ is defined as: \n",
    "\n",
    "$$idf_{t} = log_{10}(\\frac{N}{df_t})$$\n",
    "\n",
    "Note that $N$ here equals to the number of contexts.\n",
    "\n",
    "After calculating $idf$, we will calculate the tf-idf matrix. For a term $t$ and a document $d$, each term $w_{t,d}$ is defined as,\n",
    "\n",
    "$$ w_{t,d} = tf_{f,d} \\text{ x } idf_{t} $$\n",
    "\n",
    "Complete the function below that calculate the tf-idf matrix. You may not use any additional libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf(tf,df,vocab_size):\n",
    "    \"\"\"\n",
    "    This function takes a term-frequency matrix and a document frequency \n",
    "    dictionary and returns a term-frequency matrix with each word vector\n",
    "    reweighed by the idf (as defined above). \n",
    "    \n",
    "    :param tf: term frequency matrix \n",
    "    :param df: document frequency dictionary\n",
    "    :param vocab_size: size of the vocabulary\n",
    "    \n",
    "    :return tf: the reweighed tf matrix\n",
    "    \"\"\"\n",
    "    #your code goes here\n",
    "    \n",
    "    for context, word_dict in tf.items():\n",
    "        for word, count in word_dict.items():\n",
    "            word_dict[word] = count*math.log10(vocab_size/df[word])\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Lyosha': defaultdict(int,\n",
       "             {'a': 0.3881801713828814,\n",
       "              'and': 0.43933269383026263,\n",
       "              'driver': 0.7403626894942439,\n",
       "              'saxophonist': 0.8653014261025438}),\n",
       " 'Shlykov': defaultdict(int,\n",
       "             {'a': 0.3881801713828814, 'hardworking': 0.7403626894942439}),\n",
       " 'a': defaultdict(int,\n",
       "             {'Lyosha': 0.7403626894942439,\n",
       "              'Shlykov': 1.0413926851582251,\n",
       "              'and': 0.43933269383026263,\n",
       "              'bizarre': 0.7403626894942439,\n",
       "              'develop': 1.1257831106502294,\n",
       "              'hardworking': 0.7403626894942439,\n",
       "              'lovehate': 0.7403626894942439,\n",
       "              'saxophonist': 1.1257831106502294,\n",
       "              'taxi': 0.7403626894942439}),\n",
       " 'after': defaultdict(int,\n",
       "             {'all': 1.0413926851582251,\n",
       "              'different': 0.7403626894942439,\n",
       "              'so': 0.7403626894942439}),\n",
       " 'all': defaultdict(int,\n",
       "             {'after': 0.8653014261025438, 'different': 0.7403626894942439}),\n",
       " 'and': defaultdict(int,\n",
       "             {'Lyosha': 0.7403626894942439,\n",
       "              'a': 0.3881801713828814,\n",
       "              'despite': 0.7403626894942439,\n",
       "              'driver': 0.7403626894942439,\n",
       "              'lovehate': 0.7403626894942439,\n",
       "              'relationship': 0.7403626894942439,\n",
       "              'taxi': 0.7403626894942439,\n",
       "              'their': 0.7403626894942439}),\n",
       " 'arent': defaultdict(int,\n",
       "             {'different': 0.7403626894942439,\n",
       "              'realize': 0.7403626894942439,\n",
       "              'so': 0.7403626894942439,\n",
       "              'they': 0.7403626894942439}),\n",
       " 'bizarre': defaultdict(int,\n",
       "             {'a': 0.3881801713828814,\n",
       "              'develop': 0.8653014261025438,\n",
       "              'lovehate': 0.7403626894942439,\n",
       "              'relationship': 0.7403626894942439}),\n",
       " 'despite': defaultdict(int,\n",
       "             {'and': 0.43933269383026263,\n",
       "              'prejudices': 0.7403626894942439,\n",
       "              'relationship': 0.7403626894942439,\n",
       "              'their': 0.7403626894942439}),\n",
       " 'develop': defaultdict(int,\n",
       "             {'a': 0.5050340466911137,\n",
       "              'bizarre': 0.7403626894942439,\n",
       "              'saxophonist': 0.8653014261025438}),\n",
       " 'different': defaultdict(int,\n",
       "             {'after': 0.8653014261025438,\n",
       "              'all': 1.0413926851582251,\n",
       "              'arent': 0.7403626894942439,\n",
       "              'so': 0.7403626894942439}),\n",
       " 'driver': defaultdict(int,\n",
       "             {'Lyosha': 0.7403626894942439,\n",
       "              'and': 0.43933269383026263,\n",
       "              'hardworking': 0.7403626894942439,\n",
       "              'taxi': 0.7403626894942439}),\n",
       " 'hardworking': defaultdict(int,\n",
       "             {'Shlykov': 1.0413926851582251,\n",
       "              'a': 0.3881801713828814,\n",
       "              'driver': 0.7403626894942439,\n",
       "              'taxi': 0.7403626894942439}),\n",
       " 'lovehate': defaultdict(int,\n",
       "             {'a': 0.3881801713828814,\n",
       "              'and': 0.43933269383026263,\n",
       "              'bizarre': 0.7403626894942439,\n",
       "              'relationship': 0.7403626894942439}),\n",
       " 'prejudices': defaultdict(int,\n",
       "             {'despite': 0.7403626894942439,\n",
       "              'realize': 0.7403626894942439,\n",
       "              'their': 0.7403626894942439,\n",
       "              'they': 0.7403626894942439}),\n",
       " 'realize': defaultdict(int,\n",
       "             {'arent': 0.7403626894942439,\n",
       "              'prejudices': 0.7403626894942439,\n",
       "              'their': 0.7403626894942439,\n",
       "              'they': 0.7403626894942439}),\n",
       " 'relationship': defaultdict(int,\n",
       "             {'and': 0.43933269383026263,\n",
       "              'bizarre': 0.7403626894942439,\n",
       "              'despite': 0.7403626894942439,\n",
       "              'lovehate': 0.7403626894942439}),\n",
       " 'saxophonist': defaultdict(int,\n",
       "             {'Lyosha': 0.7403626894942439,\n",
       "              'a': 0.5050340466911137,\n",
       "              'develop': 0.8653014261025438}),\n",
       " 'so': defaultdict(int,\n",
       "             {'after': 0.8653014261025438,\n",
       "              'arent': 0.7403626894942439,\n",
       "              'different': 0.7403626894942439,\n",
       "              'they': 0.7403626894942439}),\n",
       " 'taxi': defaultdict(int,\n",
       "             {'a': 0.3881801713828814,\n",
       "              'and': 0.43933269383026263,\n",
       "              'driver': 0.7403626894942439,\n",
       "              'hardworking': 0.7403626894942439}),\n",
       " 'their': defaultdict(int,\n",
       "             {'and': 0.43933269383026263,\n",
       "              'despite': 0.7403626894942439,\n",
       "              'prejudices': 0.7403626894942439,\n",
       "              'realize': 0.7403626894942439}),\n",
       " 'they': defaultdict(int,\n",
       "             {'arent': 0.7403626894942439,\n",
       "              'prejudices': 0.7403626894942439,\n",
       "              'realize': 0.7403626894942439,\n",
       "              'so': 0.7403626894942439})}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST CASE\n",
    "tfidf_test = tfidf(tf_test,df_test,len(vocab_test))\n",
    "tfidf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_matrix = tfidf(tf_matrix,df_vector,VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Top K-nearest words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using cosine similarity as a measure of distance [ยง6.4 Jurafsky & Martin](https://web.stanford.edu/~jurafsky/slp3/6.pdf), we will now find the closest words to a certain word. We define cosine similarity as, $$cosine(\\overrightarrow{v},\\overrightarrow{w}) = \\frac{\\overrightarrow{v} \\cdot \\overrightarrow{w}}{\\vert v \\vert \\vert w \\vert}$$\n",
    "\n",
    "Please complete the function below that calculates the 'K' closest words from the vocabulary and returns their indices. You may not use any additional libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def k_nearest_tfidf(tfidf_matrix,word,vocab,k):\n",
    "    \"\"\"\n",
    "    The function takes in a matrix and a word and returns a list of the indices\n",
    "    of the words from the matrix which are closest to the argument word.\n",
    "    :param tfidf_matrix: the tfidf matrix\n",
    "    :param word: the word from which the distances are calculated against the vocab\n",
    "    :param vocab: the vocabulary\n",
    "    :param k: size of the results list\n",
    "    \n",
    "    :return result: the list of indices of elements, in ascending order of their distances. Size of this list = k.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    distances = np.zeros(VOCAB_SIZE)\n",
    "    \n",
    "    #your code goes here\n",
    "    \n",
    "    # calculate the length of the word vector in context\n",
    "    length_dict = {}\n",
    "    for context, word_dict in tfidf_matrix.items():\n",
    "        sum_len = 0\n",
    "        for word, tfidf in word_dict.items():\n",
    "            sum_len += tfidf**2\n",
    "        length_dict[context] = sum_len ** 0.5\n",
    "        \n",
    "    \n",
    "    for key, v_idx in vocab.items():\n",
    "        word_product = 0\n",
    "        if word in tfidf_matrix and key in tfidf_matrix:\n",
    "            for w, tfidf in tfidf_matrix[word].items():\n",
    "                if w in tfidf_matrix[key]: \n",
    "                    word_product += tfidf*tfidf_matrix[key][w]\n",
    "            \n",
    "        distances[v_idx] = word_product/(length_dict[word]*length_dict[key])\n",
    "        result = distances.argsort()[::-1][:k]\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = 'love'\n",
    "closest_words = [WORDS[i] for i in k_nearest_tfidf(tfidf_matrix,w,VOCAB,15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['told',\n",
       " 'tells',\n",
       " 'had',\n",
       " 'was',\n",
       " 'about',\n",
       " 'she',\n",
       " 'not',\n",
       " 'that',\n",
       " 'be',\n",
       " 'what',\n",
       " 'but',\n",
       " 'they',\n",
       " 'have',\n",
       " 'then',\n",
       " 'been']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-trained Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the HW, we will load pre-trained word embeddings and try to explore how the embeddings capture semantic relations. Pre-trained embeddings are word embeddings that have been trained over some corpus and are made available in the form of collection of vectors which span some space. We are going to use [Glove](https://nlp.stanford.edu/projects/glove/) embeddings which have been trained on Wikipedia and Gigaword containing over 6 Billion words. These word vectors have a dimension of 100 and we will use the top 100k most frequently occuring words from the original source. A great resource to read more about word embeddings is [this blog post](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_embeddings():\n",
    "    \"\"\"\n",
    "    Loads pretrained embeddings from a file and returns\n",
    "    the list of words, a numpy matrix with each row\n",
    "    containing the respective embedding of the word, and a \n",
    "    dictionary with key:value as word:embedding.\n",
    "    \"\"\"\n",
    "    f = open(\"glove.6B.100d.top100k.txt\",'r')\n",
    "    vocab = {}\n",
    "    words = []\n",
    "    vectors = []\n",
    "    for line in f:\n",
    "        tokens = line.split()\n",
    "        word = tokens[0]\n",
    "        embedding = np.array([float(val) for val in tokens[1:]])\n",
    "        words.append(word)\n",
    "        vectors.append(embedding)\n",
    "        vocab[word] = embedding\n",
    "    return words,np.asarray(vectors),vocab\n",
    "\n",
    "WORDS, VECTORS, VOCAB = load_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Explore the relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachment:embedding_relations.png\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding space is known to capture the semantic context of words. An example of it is $\\overrightarrow{woman} - \\overrightarrow{man} \\simeq \\overrightarrow{queen} - \\overrightarrow{king}$. We provide a function that uses the k_nearest function you wrote above to find such relations. You can have similar relations between countries and their capitals, nouns and their verb forms and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def k_nearest(matrix,vector,k):\n",
    "    \"\"\"\n",
    "    The function takes in a matrix and a normalized vector and returns the indices\n",
    "    of the vectors from the matrix which are closest to the vector.\n",
    "    The pretrained embeddings are normalized.\n",
    "    \"\"\"\n",
    "    distances = np.dot(matrix,vector)\n",
    "    result = np.argsort(distances)[::-1][:k]\n",
    "    return result\n",
    "\n",
    "def relation(word1,word2,word3):\n",
    "    \"\"\"\n",
    "    Takes in 3 words and returns the closest word (in terms of cosine similarity)\n",
    "    to the normalized algebraic addition of the three vectors.\n",
    "    The parameters follow this order : word1 - word2 ~ closest - word3\n",
    "    \"\"\"\n",
    "    if all(w in VOCAB for w in [word1,word2,word3]):\n",
    "        average = VOCAB[word1] - VOCAB[word2] + VOCAB[word3]\n",
    "        average /= np.linalg.norm(average)\n",
    "        closest = k_nearest(VECTORS,average,10)\n",
    "        for c in closest:\n",
    "            if WORDS[c] not in [word1,word2,word3]: #Find the closest word after the 3 words themselves\n",
    "                return WORDS[c]\n",
    "        return None\n",
    "    raise CustomError(\"missing word from vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation(\"woman\",\"man\",\"king\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Explore the embedding space and list down 3 interesting relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"women\" - \"man\" = \"queen\" - \"king\" is a relationship of gender; other relations that have been discovered in vector embedding space include comparative adjectives (\"good\" - \"better\" = \"rough\" - \"rougher\"), singular/plural nouns (\"year\" - \"years\" = \"law\" - \"laws\"), countries/capitals (\"Paris\" - \"France\" = \"Berlin\" - \"Germany\"), etc.  Explore the data yourself and describe 3 interesting relations (beyond these examples) that you can identify.  Points will be awarded here for creativity (discovering relations that few of your classmates discover)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter your answer here along with the respective function call to relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sea'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation(\"sky\",\"flight\",\"ship\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, flight in the sky and ship in the sea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'meat'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation(\"leaf\",\"tree\",\"pig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bad, leaf is part of tree, but meat is not part of pig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'microsoft'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation(\"apple\",\"apples\",\"bananas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bad. But this is interesting because it view 'apple' as brand name rather than a fruit type. (Apple is quite a ambiguous term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

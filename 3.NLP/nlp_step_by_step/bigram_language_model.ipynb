{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3: Language Modeling\n",
    "In this homework, you will be implementing a bigram language model on a dataset containing news headlines. Specifically, you will be tasked to complete the `count_bigrams`, `get_probability`, `sample_word`, and `calculate_perplexity` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Here we will be importing the necessary modules, as well as doing some basic preprocessing of the text. You do not need to modify any code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Reading in the training and development datasets\n",
    "with open(\"headlines.train\", 'r') as f:\n",
    "    headlines_train = f.readlines()\n",
    "with open(\"headlines.dev\", 'r') as f:\n",
    "    headlines_dev = f.readlines()\n",
    "    \n",
    "# Removing excess punctuation and newline\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "headlines_train = [regex.sub('', h.split(\"\\n\")[0]) for h in headlines_train]\n",
    "headlines_dev = [regex.sub('', h.split(\"\\n\")[0]) for h in headlines_dev]\n",
    "\n",
    "# Defining UNK, START and STOP tokens\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "START_TOKEN = \"<START>\"\n",
    "STOP_TOKEN = \"<STOP>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, let's look at what some of the headlines look like. Run the following code block as many times as you want to get a sense of what kind of headlines we hope to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gaps and delays tasmanias ailing health system\n",
      "\n",
      "cancer council victoria defends report on cfa fiskville cancer\n",
      "\n",
      "media call david furner\n",
      "\n",
      "lions unchanged for crows clash\n",
      "\n",
      "strong vic winds cause melbourne flooding\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for headline in random.sample(headlines_train, 5):\n",
    "    print(headline)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.a) Counting Bigrams\n",
    "Below we have `count_unigrams` implemented for you. Implement `count_bigrams`, which takes in a headline as text, and updates the given bigram dictionary with the bigram counts for that text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unigrams(text, unigram_dict):\n",
    "    \"\"\"\n",
    "    :param text: A headline, consisting of a string of words\n",
    "    :param unigram_dict: A dictionary containing unigrams as keys and their respective counts as values\n",
    "    \"\"\"\n",
    "    tokens = [START_TOKEN] + text.split(\" \") + [STOP_TOKEN]\n",
    "    for i in range(len(tokens)):\n",
    "        unigram = tokens[i]\n",
    "        if unigram not in unigram_dict:\n",
    "            unigram_dict[unigram] = 1\n",
    "        else:\n",
    "            unigram_dict[unigram] += 1\n",
    "\n",
    "def count_bigrams(text, bigram_dict):\n",
    "    \"\"\"\n",
    "    :param text: A headline, consisting of a string of words\n",
    "    :param bigram_dict: A dictionary containing bigrams as keys and their respective counts as values\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    tokens = [START_TOKEN] + text.split(\" \") + [STOP_TOKEN]\n",
    "    for i in range(len(tokens)-1):\n",
    "        bigram = tokens[i] + \" \" + tokens[i+1]\n",
    "        if bigram not in bigram_dict:\n",
    "            bigram_dict[bigram] = 1\n",
    "        else:\n",
    "            bigram_dict[bigram] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.b) Computing Probability\n",
    "Implement `get_probability`, which calculates the probability of seeing a word given the previous word, along with Laplace smoothing parameterized by `alpha`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability(target, context, unigram_dict, bigram_dict, alpha, vocab_size):\n",
    "    \"\"\"\n",
    "    :param target: The word whose probability of seeing is being computed given the context\n",
    "    :param context: The word directly preceeding the target word\n",
    "    :param unigram_dict: A dictionary containing unigrams as keys and their respective counts as values\n",
    "    :param bigram_dict: A dictionary containing bigrams as keys and their respective counts as values\n",
    "    :param alpha: The amount of additive smoothing being applied\n",
    "    :param vocab_size: The size of the training vocabulary\n",
    "    :return: The probability of seeing the target word given the context\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    index = context + \" \" + target\n",
    "    if index in bigram_dict:\n",
    "        return (bigram_dict[index]+alpha)/(unigram_dict[context]+alpha*vocab_size)\n",
    "    else:\n",
    "        return (1/vocab_size)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.c) Word Sampling\n",
    "Once we can calculate the desired probabilities, we can now use that to sample words for generation. Implement `sample_word`, which samples a new word in accordance with its probability of following the previous word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sample_word(words, probs):\n",
    "    \"\"\"\n",
    "    :param words: The list of words to sample from\n",
    "    :param probs: The probabilities of seeing each word; probs[i] is the probability of seeing word[i]\n",
    "    :return: A word whose sampling likelihood is the probability of being seen given the context\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pro_random = np.random.uniform(0,1)\n",
    "    p_sum = 0\n",
    "    \n",
    "    for idx, prob in enumerate(probs):\n",
    "        p_sum += prob\n",
    "        if pro_random < p_sum:\n",
    "            return words[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating New Headlines\n",
    "Now that we've all the key parts of our language model completed, let's see how well we can generate new headlines! Run the following code block to complete the algorithm, and the subsequent code block as many times as you want to see what kind of new headlines we are able to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.0001 # Change this to see different levels of smoothing affect generated text\n",
    "min_freq = 10 # The minimum word frequency to be present in the vocabulary\n",
    "\n",
    "# The following are used to keep track of and remove infrequent words\n",
    "low_freq = set()\n",
    "all_words = {}\n",
    "\n",
    "def generate_headline(unigram_dict, bigram_dict, alpha):\n",
    "    sent = [START_TOKEN]\n",
    "    while not sent[-1] == STOP_TOKEN:\n",
    "        words = list(vocab)\n",
    "        probs = [get_probability(word, sent[-1], unigram_dict, bigram_dict, alpha, len(vocab)) for word in words]\n",
    "        next_word = sample_word(words, probs)\n",
    "        sent.append(next_word)\n",
    "    print(' '.join(sent[1:-1]))\n",
    "\n",
    "def replace_text_train(text):\n",
    "    return \" \".join([UNK_TOKEN if t in low_freq else t for t in text.split()])\n",
    "\n",
    "def replace_text_dev(text):\n",
    "    return \" \".join([UNK_TOKEN if t not in vocab else t for t in text.split()])\n",
    "\n",
    "# Finding all words with low frequency\n",
    "for h in headlines_train:\n",
    "    count_unigrams(h, all_words)\n",
    "for word, count in all_words.items():\n",
    "    if count <= min_freq:\n",
    "        low_freq.add(word)\n",
    "# Replacing low frequency words from training dataset with UNK\n",
    "headlines_train_clean = [replace_text_train(h) for h in headlines_train]\n",
    "\n",
    "# Initialize unigram and bigram dictionaries\n",
    "unigrams = {}\n",
    "bigrams = {}\n",
    "\n",
    "# Generating unigram and bigram dictionaries\n",
    "for h in headlines_train_clean:\n",
    "    count_unigrams(h, unigrams)\n",
    "    count_bigrams(h, bigrams)\n",
    "    \n",
    "# Creating the training vocabulary\n",
    "vocab = set([item for sublist in map(lambda x: x.split(\" \"), headlines_train_clean) for item in sublist])\n",
    "vocab.add(START_TOKEN)\n",
    "vocab.add(STOP_TOKEN)\n",
    "\n",
    "# Replacing unseen vocabulary from development dataset with UNK\n",
    "headlines_dev_clean = [replace_text_dev(h) for h in headlines_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame\n",
      "\n",
      "suggest richard transmission possible fossil entrepreneurs expands fmg\n",
      "\n",
      "mcguigan escapees ditch bid\n",
      "\n",
      "crimea\n",
      "\n",
      "police africa\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    generate_headline(unigrams, bigrams, alpha)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.a) Calculating Perplexity\n",
    "Once we're able to generate new headlines, we can see how well our algorithm works when encountering text from an unseen development set. Implement `calculate_perplexity` below and run the code block below that to see what the perplexity of our algorithm is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(text, unigram_dict, bigram_dict, alpha, vocab_size):\n",
    "    \"\"\"\n",
    "    :param text: A headline, consisting of a string of words\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    tokens = [START_TOKEN] + text.split(\" \") + [STOP_TOKEN]\n",
    "    probs = [get_probability(tokens[i], tokens[i-1], unigram_dict, bigram_dict, alpha, len(tokens)) for i in range(1,len(tokens))]\n",
    "    p_sum = 0\n",
    "    for p in probs:\n",
    "        p_sum += math.log(p)\n",
    "    \n",
    "    return math.exp(p_sum * (-1/len(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average perplexity on dev set: 36.23\n"
     ]
    }
   ],
   "source": [
    "perplexities = []\n",
    "for h in headlines_dev_clean:\n",
    "    perp = calculate_perplexity(h, unigrams, bigrams, alpha, len(vocab))\n",
    "    perplexities.append(perp)\n",
    "print(\"Average perplexity on dev set: %.02f\" % (sum(perplexities) / len(perplexities)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.b) Parameter Experimentation\n",
    "Play around with modifying the smoothing parameter `alpha`. How does changing the values of that parameter affect the generated headlines, as well as the perplexity on the development set? Write two such observations in the cell below. Find a value for `alpha` that gives the lowest perplexities; you should be able to get a perplexity below 1000. What value of `alpha` gets the optimal perplexity value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(double click here to write your observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) RNN Language Model\n",
    "List the three headlines you generated from running the rnn_language_model.ipynb notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(double click here to write your headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

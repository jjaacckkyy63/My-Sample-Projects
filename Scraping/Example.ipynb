{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import random\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "网络爬虫     url:  /item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711\n",
      "搜索策略     url:  /item/%E6%90%9C%E7%B4%A2%E7%AD%96%E7%95%A5\n",
      "推理     url:  /item/%E6%8E%A8%E7%90%86\n",
      "阿瑟·柯南·道尔     url:  /item/%E6%9F%AF%E5%8D%97%C2%B7%E9%81%93%E5%B0%94\n",
      "肺结核     url:  /item/%E8%82%BA%E7%BB%93%E6%A0%B8\n",
      "阿瑟·柯南·道尔     url:  /item/%E6%9F%AF%E5%8D%97%C2%B7%E9%81%93%E5%B0%94\n",
      "空屋     url:  /item/%E7%A9%BA%E5%B1%8B\n",
      "失落的世界     url:  /item/%E5%A4%B1%E8%90%BD%E7%9A%84%E4%B8%96%E7%95%8C\n",
      "义项     url:  /item/%E4%B9%89%E9%A1%B9\n",
      "射雕英雄传     url:  /item/%E5%B0%84%E9%9B%95%E8%8B%B1%E9%9B%84%E4%BC%A0\n",
      "严家炎     url:  /item/%E4%B8%A5%E5%AE%B6%E7%82%8E\n",
      "人民文学出版社     url:  /item/%E4%BA%BA%E6%B0%91%E6%96%87%E5%AD%A6%E5%87%BA%E7%89%88%E7%A4%BE\n",
      "茅盾文学奖     url:  /item/%E8%8C%85%E7%9B%BE%E6%96%87%E5%AD%A6%E5%A5%96\n",
      "莫应丰     url:  /item/%E8%8E%AB%E5%BA%94%E4%B8%B0\n",
      "湖南文艺出版社     url:  /item/%E6%B9%96%E5%8D%97%E6%96%87%E8%89%BA%E5%87%BA%E7%89%88%E7%A4%BE\n",
      "湖南文艺出版社     url:  /item/%E6%B9%96%E5%8D%97%E6%96%87%E8%89%BA%E5%87%BA%E7%89%88%E7%A4%BE\n",
      "世界短篇小说精华     url:  /item/%E4%B8%96%E7%95%8C%E7%9F%AD%E7%AF%87%E5%B0%8F%E8%AF%B4%E7%B2%BE%E5%8D%8E\n",
      "湖南文艺出版社     url:  /item/%E6%B9%96%E5%8D%97%E6%96%87%E8%89%BA%E5%87%BA%E7%89%88%E7%A4%BE\n",
      "杨度     url:  /item/%E6%9D%A8%E5%BA%A6\n",
      "湖南少年歌     url:  /item/%E6%B9%96%E5%8D%97%E5%B0%91%E5%B9%B4%E6%AD%8C\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://baike.baidu.com\"\n",
    "his = [\"/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711\"]\n",
    "\n",
    "for i in range(20):\n",
    "    \n",
    "    url = base_url + his[-1]\n",
    "    \n",
    "    html = urlopen(url).read().decode('utf-8')\n",
    "    soup = BeautifulSoup(html, features='lxml')\n",
    "    print(soup.find('h1').get_text(), '    url: ', his[-1])\n",
    "    \n",
    "    sub_urls = soup.find_all(\"a\", {\"target\": \"_blank\", \"href\": re.compile(\"/item/(%.{2})+$\")})\n",
    "     \n",
    "    if len(sub_urls) != 0:\n",
    "        his.append(random.sample(sub_urls, 1)[0]['href'])\n",
    "    else:\n",
    "        # no valid sub link found\n",
    "        his.pop()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine     url:  /wiki/Machine\n",
      "Pneumatic cylinder     url:  /wiki/Pneumatic_cylinder\n",
      "Football Junior Championships of Poland     url:  /wiki/Special:Random\n",
      "Walter Brom     url:  /wiki/Walter_Brom\n",
      "Brazil     url:  /wiki/Brazil\n",
      "Brazilian Agricultural Research Corporation     url:  /wiki/Empresa_Brasileira_de_Pesquisa_Agropecu%C3%A1ria\n",
      "National Library of Australia     url:  /wiki/National_Library_of_Australia\n",
      "Canberra Museum and Gallery     url:  /wiki/Canberra_Museum_and_Gallery\n",
      "Main Page     url:  /wiki/Main_Page\n",
      "Order of the British Empire     url:  /wiki/Order_of_the_British_Empire\n",
      "Order of the Garter     url:  /wiki/Order_of_the_Garter\n",
      "John Morris, Baron Morris of Aberavon     url:  /wiki/John_Morris,_Baron_Morris_of_Aberavon\n",
      "Roy Mason     url:  /wiki/Roy_Mason\n",
      "London School of Economics     url:  /wiki/London_School_of_Economics\n",
      "Delphine Arnault     url:  /wiki/Delphine_Arnault\n",
      "20th Century Fox     url:  /wiki/20th_Century_Fox\n",
      "File:Gangs all here trailer.jpg     url:  /wiki/File:Gangs_all_here_trailer.jpg\n",
      "Help:Contents     url:  /wiki/Help:Contents\n",
      "Wikipedia:Policies and guidelines     url:  /wiki/Wikipedia:Policies_and_guidelines\n",
      "Wikipedia:Committees     url:  /wiki/Wikipedia:Committees\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://en.wikipedia.org\"\n",
    "his = [\"/wiki/Machine\"]\n",
    "\n",
    "for i in range(20):\n",
    "    \n",
    "    url = base_url + his[-1]\n",
    "    \n",
    "    html = urlopen(url).read().decode('utf-8')\n",
    "    soup = BeautifulSoup(html, features='lxml')\n",
    "    print(soup.find('h1').get_text(), '    url: ', his[-1])\n",
    "    \n",
    "    sub_urls = soup.find_all('a', {\"href\": re.compile(\"/wiki/*\")})\n",
    "    if his[-1][0] is '/':\n",
    "        new_search = random.sample(sub_urls, 1)[0]['href']\n",
    "        his.append(new_search)\n",
    "    else:\n",
    "        #no valid sub link found\n",
    "        his.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 20180628064505408.jpg\n",
      "Saved 20180628053326585.jpg\n",
      "Saved 20180627101525881.jpg\n",
      "Saved 20180625040726991.jpg\n",
      "Saved 20180625111440934.jpg\n",
      "Saved 20180622110424583.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.makedirs('./img/', exist_ok=True)\n",
    "\n",
    "URL = 'http://www.nationalgeographic.com.cn/animals/'\n",
    "\n",
    "html = requests.get(URL).text\n",
    "soup = BeautifulSoup(html, features='lxml')\n",
    "\n",
    "img_ul = soup.find_all('ul', {'class':\"img_list\"})\n",
    "\n",
    "for ul in img_ul:\n",
    "    imgs = ul.find_all('img')\n",
    "    for img in imgs:\n",
    "        url = img['src']\n",
    "        r = requests.get(url, stream=True)\n",
    "        image_name = url.split('/')[-1]\n",
    "        \n",
    "        with open('./img/%s' % image_name, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=128):\n",
    "                f.write(chunk)\n",
    "        print('Saved %s' % image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
